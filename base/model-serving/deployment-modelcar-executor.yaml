# Synesis Executor â€” ModelCar + ECR deployment (RHOAI 3.2 / Blackwell)
#
# Model pulled from ECR (mirrored via scripts/mirror-models-to-ecr.sh).
# Uses /dev/shm (32Gi) and vllm-sockets for IPC/UDS. VPC endpoint for ECR.
#
# Prerequisites:
#   - ECR repo synesis-models with image :executor
#   - ImagePullSecret for ECR (or IRSA on worker nodes)
#   - GPU node (e.g. G7e.2xlarge, Blackwell)
#
# Apply: oc apply -f base/model-serving/deployment-modelcar-executor.yaml -n synesis-models
# Override image: envsubst < base/model-serving/deployment-modelcar-executor.yaml | oc apply -f - -n synesis-models
---
apiVersion: v1
kind: Service
metadata:
  name: synesis-executor-predictor
  namespace: synesis-models
  labels:
    app.kubernetes.io/name: synesis-executor
    app.kubernetes.io/component: model
spec:
  selector:
    app: synesis-executor
  ports:
    - name: http
      port: 8080
      targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: synesis-executor-predictor
  namespace: synesis-models
  labels:
    app.kubernetes.io/name: synesis-executor
    app.kubernetes.io/component: model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: synesis-executor
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: synesis-executor
        app.kubernetes.io/name: synesis-executor
    spec:
      # imagePullSecrets for ECR; use IRSA instead if worker nodes have it
      # imagePullSecrets:
      #   - name: ecr-registry
      imagePullPolicy: IfNotPresent
      volumes:
        - name: model-store
          emptyDir: {}
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
        - name: vllm-sockets
          emptyDir: {}
      initContainers:
        - name: fetch-model
          image: ${ECR_REGISTRY:-123456789012.dkr.ecr.us-east-1.amazonaws.com}/synesis-models:executor
          command: ["/bin/sh", "-c", "cp -a /models/. /model/"]
          volumeMounts:
            - name: model-store
              mountPath: /model
      containers:
        - name: vllm-executor
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a
          imagePullPolicy: IfNotPresent
          command:
            - python
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --port=8080
            - --model=/mnt/models
            - --served-model-name=synesis-executor
            - --tensor-parallel-size=1
            - --max-model-len=32768
            - --gpu-memory-utilization=0.55
            - --enable-prefix-caching
            - --enable-chunked-prefill
            - --reasoning-parser=qwen3
            - --enable-auto-tool-choice
            - --tool-call-parser=qwen3_coder
            - --trust-remote-code
          env:
            - name: HF_HOME
              value: /tmp/hf_home
          resources:
            limits:
              cpu: "8"
              memory: 48Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: 32Gi
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-store
              mountPath: /mnt/models
              readOnly: true
            - name: shm
              mountPath: /dev/shm
            - name: vllm-sockets
              mountPath: /tmp/vllm
      # GPU node selector (adjust for Blackwell / G7e)
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-L40S
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: "true"
