# Synesis Supervisor â€” ModelCar + ECR (Qwen3-8B or 0.5B)
# Same pattern as executor; lighter resources.
---
apiVersion: v1
kind: Service
metadata:
  name: synesis-supervisor-predictor
  namespace: synesis-models
  labels:
    app.kubernetes.io/name: synesis-supervisor
spec:
  selector:
    app: synesis-supervisor
  ports:
    - name: http
      port: 8080
      targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: synesis-supervisor-predictor
  namespace: synesis-models
  labels:
    app.kubernetes.io/name: synesis-supervisor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: synesis-supervisor
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: synesis-supervisor
    spec:
      imagePullPolicy: IfNotPresent
      volumes:
        - name: model-store
          emptyDir: {}
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
        - name: vllm-sockets
          emptyDir: {}
      initContainers:
        - name: fetch-model
          image: ${ECR_REGISTRY:-123456789012.dkr.ecr.us-east-1.amazonaws.com}/synesis-models:supervisor
          command: ["/bin/sh", "-c", "cp -a /models/. /model/"]
          volumeMounts:
            - name: model-store
              mountPath: /model
      containers:
        - name: vllm-supervisor
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a
          imagePullPolicy: IfNotPresent
          command:
            - python
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --port=8080
            - --model=/mnt/models
            - --served-model-name=synesis-supervisor
            - --tensor-parallel-size=1
            - --max-model-len=32768
            - --gpu-memory-utilization=0.15
            - --enable-prefix-caching
          resources:
            limits:
              cpu: "4"
              memory: 8Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "2"
              memory: 4Gi
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-store
              mountPath: /mnt/models
              readOnly: true
            - name: shm
              mountPath: /dev/shm
            - name: vllm-sockets
              mountPath: /tmp/vllm
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-L40S
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: "true"
